{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Lab: Week 3\n",
    "\n",
    "This week we are going to learn about **NumPy** and manipulating 1D NumPy arrays.  There is also a *debug* challenge and a '*do something we haven't taught you how to do*' challenge at the end of the lab.\n",
    "\n",
    "The lab assumes you are using `Spyder 3` and Python 3.x\n",
    "\n",
    "You should have attended Lecture 3 in the Python series and read the lecture notes before attempting this lab.\n",
    "\n",
    "**Remember for any code where you wish to use numpy you need to import it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Create NumPy arrays and accessing data\n",
    "\n",
    "The fundermental building block in NumPy is the `numpy.ndarry`\n",
    "\n",
    "As we discovered in the last lecture arrays are quite different from a Python `List`.  However, creation and accessing individual array items and slicing array is very similar to a list.  A big difference is that a `numpy.ndarray` requires\n",
    "all data values to be of the **same type**.\n",
    "\n",
    "Suppose that we want a numpy array containing the integers 4, 3, 1, 5 and 6.\n",
    "\n",
    "A simple way to create such an array and access its data is to use the following syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the array contains [4 3 1 5 6]\n",
      "a numpy array has has type <class 'numpy.ndarray'>\n",
      "the array has a shape of (5,)\n",
      "The item at index 0 in the array is 4\n",
      "The item at index 2 in the array is 1\n",
      "If we slice the array between item 0 and 2 we get [4 3]\n",
      "If we slice the array between item 3 and 5 we get [5 6]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([4, 3, 1, 5, 6])\n",
    "print('the array contains {0}'.format(arr))\n",
    "print('a numpy array has has type {0}'.format(type(arr)))\n",
    "print('the array has a shape of {0}'.format(arr.shape))\n",
    "print('The item at index 0 in the array is {0}'.format(arr[0]))\n",
    "print('The item at index 2 in the array is {0}'.format(arr[2]))\n",
    "print('If we slice the array between item 0 and 2 we get {0}'.format(arr[:2]))\n",
    "print('If we slice the array between item 3 and 5 we get {0}'.format(arr[3:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Creating empty arrays\n",
    "    \n",
    "* The size of a NumPy array needs to be defined upfront.  \n",
    "* You cannot dynamically append to a `numpy.ndarray` like you can with a `List`\n",
    "* One option is to create an empty numpy array or a numpy array containing all zeros (if using numeric data).\n",
    "* When you create an 'empty' array you are allocated a space in the computer's memory.\n",
    "* There may be some strange data in inside (note your output might look different to the below depending on what is in your computer's memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.94870485e-310 4.67787424e-310 6.94867951e-310 6.94870505e-310\n",
      " 6.94870234e-310 6.94870505e-310 6.94870502e-310 6.94867951e-310\n",
      " 6.94867951e-310 6.94867951e-310]\n",
      "<class 'numpy.ndarray'>\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "empty_arr = np.empty(10)\n",
    "print(empty_arr)\n",
    "print(type(empty_arr))\n",
    "print(empty_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternatively you could create an array that contains all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "<class 'numpy.ndarray'>\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "zeros_arr = np.zeros(10)\n",
    "print(zeros_arr)\n",
    "print(type(zeros_arr))\n",
    "print(zeros_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Manipulating array data\n",
    "\n",
    "* It is trivial to update individual elements in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "updated data [  0.   0.   0.   0.   0. 111.   0.   0.   0. 222.]\n"
     ]
    }
   ],
   "source": [
    "data = np.zeros(10)\n",
    "print('original data {0}'.format(data))\n",
    "\n",
    "data[5] = 111\n",
    "data[9] = 222\n",
    "print('updated data {0}'.format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When you slice an array you effectively create a **view** of the data.\n",
    "* This means that when you update the slice you update the original array as well.\n",
    "* Numpy keeps the data in the same place in memory for efficiency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "slice of data [999. 999. 999.]\n",
      "the original data is also updated [  0.   0.   0. 999. 999. 999.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "data = np.zeros(10)\n",
    "print('original data {0}'.format(data))\n",
    "\n",
    "slice_of_data = data[3:6] # slice from index 3 to index 5\n",
    "slice_of_data += 999 #incremement each value in the slice by 999\n",
    "print('slice of data {0}'.format(slice_of_data))\n",
    "print('the original data is also updated {0}'.format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NumPy makes basic matrix alegbra simple and efficient\n",
    "* Here we are going to work with 2 arrays of the same size.\n",
    "* We are going to create a sequence of numbers in each using the `np.arange()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1: [0 1 2 3 4 5 6 7 8 9]\n",
      "data2: [10 11 12 13 14 15 16 17 18 19]\n",
      "The square of each value in data1 [ 0  1  4  9 16 25 36 49 64 81]\n",
      "The summation of the the two arrays [10 12 14 16 18 20 22 24 26 28]\n",
      "The difference between the the two arrays [-10 -10 -10 -10 -10 -10 -10 -10 -10 -10]\n",
      "If we add 10 to each element in data1 we get [10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "data1 = np.arange(10)  #create a sequence of integers 0-9\n",
    "data2 = np.arange(10,20) #create a sequence of integers 10-19\n",
    "\n",
    "print('data1: {0}'.format(data1))\n",
    "print('data2: {0}'.format(data2))\n",
    "\n",
    "print('The square of each value in data1 {0}'.format(data1**2)) \n",
    "print('The summation of the the two arrays {0}'.format(data1 + data2))\n",
    "print('The difference between the the two arrays {0}'.format(data1 - data2))\n",
    "\n",
    "data1 += 10\n",
    "print('If we add 10 to each element in data1 we get {0}'.format(data1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create and manipulate your own numpy array\n",
    "    \n",
    "* Create two numpy arrays of size 10. \n",
    "* The first array should be called `array_1` have all zero values\n",
    "* The second array `array_2` should be a sequence from 90 to 99\n",
    "* Create a slice of `array_1` to access the last 5 elements of the array.\n",
    "* Add the value 10 to each of these slices\n",
    "* Now multiply the two arrays together and print out the result\n",
    "\n",
    "The expected result is:\n",
    "\n",
    "```python\n",
    "[0, 0, 0, 0, 0, 950, 960, 970, 980, 990]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy for data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic statistical analysis of the Standard Normal\n",
    "\n",
    "We are going to explore the basic analysis capabilities of numpy.\n",
    "\n",
    "The first thing we will do is generate some synthetic data.  We are going to take 10000 random samples from the standard normal distribution.  Numpy has a function to do this called `numpy.random.randn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = np.random.randn(10000)  \n",
    "print(data.shape)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then going to create a function to conduct and report a descriptive analysis of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.004860055017635998, 0.9873939059870946, -2.314439717295787, 2.2606790556874214)\n"
     ]
    }
   ],
   "source": [
    "def descriptives(data):\n",
    "    \"\"\"\n",
    "    Returns mean, stdev, and 1st and 99th \n",
    "    percentile of a 1D numpy.array\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- 1d numpy.ndarray containing data to analyse\n",
    "    \"\"\"\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    per_1st = np.percentile(data, 1) \n",
    "    per_99th = np.percentile(data, 99)\n",
    "    \n",
    "    return mean, std, per_1st, per_99th\n",
    "\n",
    "\n",
    "results = descriptives(data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the mean of the distribution is approx zero with a standard deviation of 1. \n",
    "\n",
    "We can also see that 99% of our data lie between -2.3 and +2.3 (which we would expect from the standard normal)\n",
    "\n",
    "It is very simple to work with NumPy arrays containing numeric data.  For example if we wanted to find all of our samples that are greater than or equal to +2.3 we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "<class 'numpy.ndarray'>\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "result = data >= 2.3\n",
    "\n",
    "print(result.shape)\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code returns a new `numpy.ndarray` that contains boolean (True/False) values.  The value at array index `i` is `True` if the corresponding value at index `i` in array `data` is >= 2.3 and `False` otherwise.  If we had used a python `List` we would have needed to loop through all of list items and perform the check ourselves.\n",
    "\n",
    "Let's create some generalised functions to return the probabilities that a value is greater or less than a user specified value in our data set.\n",
    "\n",
    "To do that we need to know that \n",
    "\n",
    "```python\n",
    "False == 0\n",
    "True == 1\n",
    "```\n",
    "\n",
    "Therefore we can simply the sum of our boolean array to find out how many array elements are greater or less than a user specified values e.g.\n",
    "\n",
    "```python\n",
    "(data >= 2.3).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0242 0.0238\n"
     ]
    }
   ],
   "source": [
    "def prob_great_than_or_equal_to(data, x):\n",
    "    '''\n",
    "    Return the proportion of the dataset that\n",
    "    is greater than or equal to x\n",
    "    \n",
    "    Keyword arguments\n",
    "    data -- a numpy.ndarray containing numeric data\n",
    "    x -- a numeric value. Function returns proportion where data >=x\n",
    "    '''\n",
    "    return (data >= x).sum()/data.shape[0]\n",
    "\n",
    "\n",
    "def prob_less_than_or_equal_to(data, x):\n",
    "    '''\n",
    "    Return the proportion of the dataset that\n",
    "    is less than or equal to x\n",
    "    \n",
    "    Keyword arguments\n",
    "    data -- a numpy.ndarray containing numeric data\n",
    "    x -- a numeric value. Function returns proportion where data <=x\n",
    "    '''\n",
    "    return (data <= x).sum()/data.shape[0]\n",
    "\n",
    "x1 = prob_great_than_or_equal_to(data, 1.96)\n",
    "x2 = prob_less_than_or_equal_to(data, -1.96)\n",
    "\n",
    "print(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test of these functions shows use that around 95% of data lie between points -1.96 and +1.96 (which again we would expect with the standard normal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Simple linear regression using data in numpy arrays \n",
    "    \n",
    "NumPy arrays are the fundermental building block of the Python SciPy stack. \n",
    "Scientific computing in Python nearly always makes use of `numpy.ndarrays` at some level.\n",
    "\n",
    "In this example we will load two NumPy arrays from file and conduct a simple linear regression.  The method of Ordinary Least Squares is used to fit a linear model (think $y = \\beta_1 x + \\beta_0 + \\epsilon $ ) to some data stored in numpy arrays.\n",
    "\n",
    "We have two datasets.\n",
    "\n",
    "* `breach.csv`: monthly totals of people waiting 4 or more hours in English emergency departments\n",
    "* `dtocs.csv`: monthly total of the number of people waiting to be discharged (leave) hospial and go home or transfer to another form of healthcare.  \n",
    "\n",
    "We are going to (naively) assess the relationship between these two variables.  For the purposes of this example we are going to ignore that these two datasets are time-series data.\n",
    "\n",
    "The library we will use to conduct linear regression is `statsmodels.api`\n",
    "We will use the function `OLS` which accepts two keyword arguments: y and x\n",
    "\n",
    "Once we have conducted the analysis we will print the results summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.714\n",
      "Model:                            OLS   Adj. R-squared:                  0.710\n",
      "Method:                 Least Squares   F-statistic:                     194.6\n",
      "Date:                Thu, 24 Jan 2019   Prob (F-statistic):           6.80e-23\n",
      "Time:                        21:22:00   Log-Likelihood:                -945.02\n",
      "No. Observations:                  80   AIC:                             1894.\n",
      "Df Residuals:                      78   BIC:                             1899.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const      -1.633e+05      2e+04     -8.178      0.000   -2.03e+05   -1.24e+05\n",
      "x1            57.7282      4.138     13.950      0.000      49.489      65.967\n",
      "==============================================================================\n",
      "Omnibus:                       11.472   Durbin-Watson:                   0.888\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):               16.361\n",
      "Skew:                           0.591   Prob(JB):                     0.000280\n",
      "Kurtosis:                       4.874   Cond. No.                     2.61e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.61e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm  #this contains the analysis function we will use\n",
    "\n",
    "def load_dtoc_dataset():\n",
    "    '''\n",
    "    Loads the breach and dtoc data sets into memory\n",
    "    Returns a tuple of numpy.ndarrays representing\n",
    "    breach and dtoc dataset respectively.\n",
    "    '''\n",
    "    #note we use skip_header because the dataset has column descriptors\n",
    "    dtoc = np.genfromtxt('dtocs.csv', skip_header=1)  \n",
    "    breach = np.genfromtxt('breach.csv', skip_header=1)\n",
    "    return breach, dtoc\n",
    "    \n",
    "    \n",
    "breach, dtoc = load_dtoc_dataset()\n",
    "\n",
    "#regression code\n",
    "dtoc = sm.add_constant(dtoc) # an intercept term to the model\n",
    "model = sm.OLS(breach, dtoc)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The results of regression analysis gives an adjusted R-squared of 0.71\n",
    "* Both the intercept (cont) and dtocs (x1) have confidence intervals that do not include zero\n",
    "* Our analysis suggests that (on the face of it) there is an association between the two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Descriptive analysis of hourly banks customers\n",
    "\n",
    "The dataset `bank_arrivals.csv` contains 1000 hourly observations of customers arriving at a bank cashiers queue.\n",
    "\n",
    "* Load the bank arrivals from the file into a numpy array\n",
    "* Check that you have successfully loaded all 1000 observations\n",
    "* Create a function `descriptives` that accepts an numpy array as an argument.\n",
    "* The function should calculate and return the mean, standard deviation, median and inter-quartile range\n",
    "* **Tip**: The inter-quartile range is the difference between the 75th and 25th percentile; the median is the 50th percentile.\n",
    "* Calculate the mean, standard deviation, median and inter-quartile range for bank arrivals data\n",
    "* Print the summary statistics to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(9.987, 9.848831, 10.0, 4.0)\n"
     ]
    }
   ],
   "source": [
    "def descriptives(data):\n",
    "    \"\"\"\n",
    "    Returns mean, var, median and IQR of a 1D numpy.array\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- 1d numpy.ndarray containing data to analyse\n",
    "    \"\"\"\n",
    "    mean = data.mean()\n",
    "    var = data.var()\n",
    "    median = np.percentile(data, 50) \n",
    "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    \n",
    "    return mean, var, median, iqr\n",
    "\n",
    "arrival_data = np.genfromtxt('bank_arrivals.csv', delimiter=',')\n",
    "print(arrival_data.shape)\n",
    "\n",
    "summary_stats = descriptives(arrival_data)\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Before and After Analysis of Stroke Treatment Rates\n",
    "\n",
    "The file `lysis.csv` contains a monthly proportion of a hospital's stroke patients that have been treated with a potentially life saving drug to remove a blood clot from their brain. \n",
    "\n",
    "There are a total of 54 months in the dataset.\n",
    "\n",
    "In month 42 the hospital introduced a new process for fast tracking patients to treatment.  Your task is to quantify the difference in treatment rates before and after the fast track intervention and to determine if the result is statistically significant.\n",
    "\n",
    "To do this you need to:\n",
    "\n",
    "* import the OLS function from `statsmodels.api`\n",
    "* Read the `lysis.csv` file into a `numpy.ndarray` variable (hint: watch out for headers in the file)\n",
    "* Create a numpy array that is the same size as the lysis array. It should contain zero when it the month (index) is less than when the intervention was introduced (42) and 1 for all months after the intervention was introduced.  Your array should look like\n",
    "\n",
    "```python\n",
    "dummy == [0,0,0, ... ,1,1,1] \n",
    "\n",
    "# Hints:\n",
    "# dummy.shape[0] == 54;  (i.e. its length is 54)\n",
    "# where index < 42 dummy[index] = 0; where index >=42 dummy[index] == 1\n",
    "```\n",
    "\n",
    "\n",
    "* Conduct a regression analysis of the before and after period and display the results.\n",
    "* Tip: remember that the regression analysis should include a constant term.\n",
    "* The independent variables in your regression is the dummy variable\n",
    "* The dependent variables in your regression is the data read in from `lysis.csv`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over NumPy Arrays\n",
    "\n",
    "### Example 1: Basic Iteration over an array\n",
    "\n",
    "* For a 1D `numpy.ndarray` you can iterate (loop) over each element much in the same way you would iterate over items in a `List`\n",
    "* Here we create an array with 10 items - the numbers 0 to 9.\n",
    "* We then iterate over each item using a `for` loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 "
     ]
    }
   ],
   "source": [
    "data = np.arange(10)\n",
    "\n",
    "for x in data:\n",
    "    print(x, end= ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternatively a `for` loop and array index approach could be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 "
     ]
    }
   ],
   "source": [
    "data = np.arange(10)\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    print(data[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Finding the maximum value in an array\n",
    "\n",
    "A simple example of a loop is finding the maximum value in an array.  This works in a similar fashion to when \n",
    "using a python `List`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "def max_value(data):\n",
    "    '''\n",
    "    Loop over each element in array\n",
    "    and return the maximum value\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data -- numpy.array containing numeric data\n",
    "    '''\n",
    "    max_value = 0\n",
    "    \n",
    "    for x in data:\n",
    "        max_value = max(x, max_value)\n",
    "    return max_value\n",
    "        \n",
    "data = np.array([100, 10, 10, 1000, 999, 1])\n",
    "print(max_value(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in practice you would use numpy's build-it function for max as opposed to writing your own code.\n",
    "\n",
    "```python\n",
    "data = np.array([100, 10, 10, 1000, 999, 1])\n",
    "print(data.max())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Vectorizing a function\n",
    "\n",
    "* NumPy allows you to vectorize a function.  \n",
    "* This means that you can apply a function to every item in an array without requiring an explicit `for` loop in your code.\n",
    "\n",
    "As an example consider the following array:\n",
    "```python\n",
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "```\n",
    "\n",
    "If we wanted to limit the values in this array to a maximum value of 5 then one way to do this is to use an explicit `for` loop to iterate over all of the elements and impose a ceiling on the value.  I.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [0 1 2 3 4 5 6 7 8 9]\n",
      "data with ceiling [0 1 2 3 4 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "def ceiling(arr, upper_limit):\n",
    "    '''\n",
    "    Loop through all elements of a np.ndarray\n",
    "    and impose a max_value ceiling on the data.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    arry - numeric np.ndarray to iterate\n",
    "    upper_limit - the numeric upper limit on values in arr\n",
    "    '''\n",
    "    for i in range(arr.shape[0]):\n",
    "        arr[i] = min(arr[i], upper_limit)\n",
    "        \n",
    "data = np.arange(10)\n",
    "\n",
    "print('original data: {0}'.format(data))\n",
    "\n",
    "ceiling(data, 5)\n",
    "\n",
    "print('data with ceiling {0}'.format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, with NumPy we could instead make use of the `np.vectorize()` function\n",
    "* To make it work we need to update the `ceiling` function so that it works on individual array elements\n",
    "* This means that we no longer have a `for` loop in our code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [0 1 2 3 4 5 6 7 8 9]\n",
      "data with ceiling [0 1 2 3 4 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "def ceiling(to_test, upper_limit):\n",
    "    '''\n",
    "    Returing the minimum value by\n",
    "    comparing to_test to max_value\n",
    "    \n",
    "    Keyword arguments:\n",
    "    to_test - numeric value to test if breaches ceiling\n",
    "    upper_limit - the numeric upper limit on to_test\n",
    "    '''\n",
    "    return min(to_test, upper_limit)\n",
    "\n",
    "# v_ceiling is a wrapper function that we call instead of ceiling\n",
    "v_ceiling = np.vectorize(ceiling)  \n",
    "\n",
    "data = np.arange(10)\n",
    "c_data = v_ceiling(data, 5)\n",
    "\n",
    "print('original data: {0}'.format(data))\n",
    "print('data with ceiling {0}'.format(c_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this simple example the function `ceiling` is just a wrapper for the the built in function `min`\n",
    "* Therefore we could vectorize `min` instead. \n",
    "* This means we need even less code again!\n",
    "* In reality you are more likely to vectorize your own custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [0 1 2 3 4 5 6 7 8 9]\n",
      "data with ceiling [0 1 2 3 4 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# here we vectorize min instead of our custom function\n",
    "v_ceiling = np.vectorize(min)  \n",
    "\n",
    "data = np.arange(10)\n",
    "c_data = v_ceiling(data, 5)\n",
    "\n",
    "print('original data: {0}'.format(data))\n",
    "print('data with ceiling {0}'.format(c_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Using np.where() and fancy indexing\n",
    "\n",
    "Another efficient alternative to using a loop in NumPy is to use `np.where()`\n",
    "\n",
    "* `np.where()` is like asking \"tell me where in this array, values satisfy a given condition\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([3, 4]),)\n"
     ]
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 500, 700])\n",
    "results = np.where(data > 2)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `results` above tell us that the array `data` contains values that are > 3 in indexes 3 and 4\n",
    "* This might be a useful result on its own.  \n",
    "* To go one step further and access and manipulate the values in indexes 3 and 4 we need to use fancy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 500 700]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 500, 700])\n",
    "indexes = [1, 3, 4]\n",
    "sliced_data = data[indexes]\n",
    "\n",
    "print(sliced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code above demonstrates fancy indexing.  \n",
    "* We defined an `List` of indexes i.e. `indexes = [1, 3, 4]` (this could also be another `np.ndarray`)\n",
    "* The code `data[indexes]` 'looks up' the values contained in indexes 1, 3, and 4 of the array `data`\n",
    "* If we combine using `np.where()` and 'fancy indexing' we have very powerful code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500 700]\n",
      "[  0   1   2 999 999]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 500, 700])\n",
    "sliced_data = data[np.where(data > 2)]\n",
    "\n",
    "print(sliced_data)\n",
    "\n",
    "data[np.where(data > 2)] = 999 \n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can implement the `ceiling` function from the previous example using np.where()\n",
    "* It is neat solution that requires minimal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [0 1 2 3 4 5 6 7 8 9]\n",
      "data with ceiling [0 1 2 3 4 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "def ceiling(data, upper_limit):\n",
    "    '''\n",
    "    Returing the minimum value by\n",
    "    comparing to_test to max_value\n",
    "    \n",
    "    Keyword arguments:\n",
    "    to_test - numeric value to test if breaches ceiling\n",
    "    upper_limit - the numeric upper limit on to_test\n",
    "    '''\n",
    "    data[np.where(data > upper_limit)] = upper_limit\n",
    "\n",
    "data = np.arange(10)\n",
    "\n",
    "print('original data: {0}'.format(data))\n",
    "ceiling(data, 5)\n",
    "print('data with ceiling {0}'.format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Calculate the winsorized mean\n",
    "    \n",
    "* Assume you have the data \n",
    "\n",
    "```python\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "```        \n",
    "* The 20% Winsorized mean of `data` is calculated on a modified data set where the top and bottom 10% of values are replaced by the 10th and 90th percentiles.  \n",
    "* In this case the 10th percentile = 2 and the 90th = 10.  Therefore the winsorized dataset is:\n",
    "\n",
    "```python\n",
    "win_data = [2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]\n",
    "\n",
    "win_mean = win_data.mean()\n",
    "```\n",
    "* Write a function `winsorise(data, cut_off = 0.1)`\n",
    "* The function must modify the numeric np.ndarray `data` so that is it is winsorised.  \n",
    "* A cut_off = 0.1 specifies that the function uses the 10th and 90th percentiles as cut-offs\n",
    "\n",
    "Hints:\n",
    "\n",
    "* There are multiple ways to solve this problem\n",
    "    * You could use a `for` loop\n",
    "    * You could create a function that is vectorized using `np.vectorize()`\n",
    "    * You could use `np.where()` and fancy indexing\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3: Debug Challenge\n",
    "\n",
    "Each laboratory will have a debug challenge.  You will be given a pre-existing script containing Python code. The catch is that the code doesn't run!  \n",
    "\n",
    "Your challenge is to find and correct the errors so that the script correctly executes.  \n",
    "\n",
    "The challenges are based around common problems students have when writing code.  If you do the exercises it will help you debug your own code and maybe even avoid the mistakes in the first place!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1: \n",
    "\n",
    "This weeks debug challenge is fixing a monte-carlo simulation model. \n",
    "\n",
    "We return to our stroke care example from earlier in the lab.  This time rather than conducting \n",
    "an empricial analysis we are going to model each stage of the emeregency stroke care pathway\n",
    "and examine the distribution of performance that might be expected.\n",
    "\n",
    "The model is a simplification of a stroke treatment pathway at a hospital.  \n",
    "Patients must be treated within 180 minutes of the onset of their symptoms.\n",
    "\n",
    "* They must be brought to hospital\n",
    "* be seen in the ED\n",
    "* undego a CT scan\n",
    "* be clinically assessed by a stroke medic\n",
    "* not have any additional illnesses that prevent treatment\n",
    "\n",
    "Instructions:\n",
    "* open `wk3_debug_challenge.py` in `spyder 3`.  \n",
    "* Attempt to run the code.  The code will raise errors.\n",
    "* Fix the bugs!\n",
    "\n",
    "Hints: \n",
    "* Read the Python interpreter output.  \n",
    "* The errors reported can look confusing at first, but read them carefully and they will point you to the lines of code with problems.\n",
    "* The `Spyder` IDE may give you some hints about formatting errors\n",
    "* It can be useful to use `print()` to display intermediate calculations and variable values.\n",
    "* Remember that `Spyder` has a variable viewer where you can look at the value of all variables created.  \n",
    "* There might be multiple bugs!  When you fix one and try to run the code you might find another!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2: Do something we haven't taught you how to do challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1: Preprocessing data to detrend a timeseries \n",
    "\n",
    "In example 2, we conducted a simple linear regression to assess the relationship between two variables.  Our initial analysis is problematic because both variables are time series and contain autocorrelation and trend.  This means that we violate some of the assumptions of ordinary least squares regression and our estimates of the relationship are likely to be incorrect.\n",
    "\n",
    "In practice, we would pre-process the data in the numpy arrays before conducting the regression.  A simple way to do this is to take the first difference of the time series. If $Y_t$ represents the value of the time series at time $t$ then the first difference is equal to:\n",
    "\n",
    "$$Y_t = Y_{t+1} - Y_t$$ \n",
    "\n",
    "**Tip**:  If an array $a$ has length $n$ then an array of the first differences of $a$ is $n-1$\n",
    "\n",
    "**Task**:\n",
    "\n",
    "* Copy the code from Example 2.\n",
    "* Modify the code to take the first difference of the breach and dtoc numpy arrays\n",
    "* Conduct the regression analysis using the first differences (detrended data)\n",
    "* Look at the new $R^2$.  Is there still strong evidence of a relationship?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
